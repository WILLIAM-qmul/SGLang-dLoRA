"""
ç®€åŒ–ç‰ˆ LoRA Benchmark è„šæœ¬
æµ‹è¯•åœºæ™¯ï¼š
1. 8ä¸ªè¯·æ±‚éƒ½ä½¿ç”¨ lora0 (åŒä¸€LoRA)
2. 8ä¸ªè¯·æ±‚éšæœºä½¿ç”¨ä¸åŒLoRA (æ··åˆLoRA)
"""

import argparse
import asyncio
import aiohttp
import json
import random
import sys
import time
import traceback
from typing import List, Optional, Tuple
from dataclasses import dataclass
from tqdm. asyncio import tqdm
from transformers import PreTrainedTokenizerBase, AutoTokenizer

from sglang.bench_serving import (
    _create_bench_client_session,
    RequestFuncInput,
    RequestFuncOutput,
    remove_prefix,
)

from sglang.srt. instances.lora_config_paths import LORA_PATH, NUM_LORAS


@dataclass
class BenchmarkMetrics:
    """Benchmark æ€§èƒ½æŒ‡æ ‡"""
    scenario: str
    num_requests: int
    completed: int
    failed: int
    total_time_s: float
    mean_latency_ms: float
    median_latency_ms: float
    p95_latency_ms: float
    p99_latency_ms: float
    mean_ttft_ms: float
    median_ttft_ms: float
    total_input_tokens: int
    total_output_tokens: int
    throughput_rps: float
    output_throughput_tps: float


def get_tokenizer(model_path: str) -> PreTrainedTokenizerBase:
    """è·å– tokenizer"""
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    return tokenizer


def prepare_test_requests(
    num_requests: int = 8,
    prompt_len: int = 256,
    output_len: int = 128
) -> List[Tuple[str, int, int]]:
    """
    å‡†å¤‡æµ‹è¯•è¯·æ±‚
    
    Returns:
        List of (prompt, prompt_len, output_len)
    """
    requests = []
    
    # ç”Ÿæˆç®€å•çš„æµ‹è¯• prompts
    test_prompts = [
        "Write a short story about artificial intelligence.",
        "Explain the concept of machine learning in simple terms.",
        "What are the benefits of using LoRA for model fine-tuning?",
        "Describe a futuristic city powered by renewable energy.",
        "How does natural language processing work?",
        "Explain quantum computing to a high school student.",
        "What is the future of autonomous vehicles?",
        "Describe the process of training a neural network.",
    ]
    
    for i in range(num_requests):
        prompt = test_prompts[i % len(test_prompts)]
        requests.append((prompt, prompt_len, output_len))
    
    return requests


async def async_request_sglang(
    request_func_input: RequestFuncInput,
    pbar: Optional[tqdm] = None,
) -> RequestFuncOutput:
    """å‘ SGLang æœåŠ¡å™¨å‘é€è¯·æ±‚"""
    api_url = request_func_input.api_url
    prompt = request_func_input.prompt
    
    async with _create_bench_client_session() as session:
        payload = {
            "text": prompt,
            "sampling_params": {
                "max_new_tokens": request_func_input.output_len,
                "temperature": 0.0,  # ç¡®å®šæ€§è¾“å‡º
            }
        }
        
        # å¦‚æœæŒ‡å®šäº† LoRAï¼Œæ·»åŠ åˆ° payload
        if request_func_input.lora_name:
            payload["lora_path"] = request_func_input. lora_name
        
        headers = {"Content-Type": "application/json"}
        
        output = RequestFuncOutput()
        output.prompt_len = request_func_input.prompt_len
        
        generated_text = ""
        ttft = 0.0
        st = time.perf_counter()
        most_recent_timestamp = st
        
        try: 
            async with session.post(
                url=api_url, 
                json=payload, 
                headers=headers
            ) as response:
                if response.status == 200:
                    async for chunk_bytes in response.content:
                        chunk_bytes = chunk_bytes.strip()
                        if not chunk_bytes:
                            continue
                        
                        chunk = remove_prefix(
                            chunk_bytes. decode("utf-8"), 
                            "data: "
                        ).strip()
                        
                        if chunk == "[DONE]":
                            break
                        
                        try:
                            data = json. loads(chunk)
                        except Exception: 
                            continue
                        
                        # æ£€æŸ¥é”™è¯¯
                        if isinstance(data, dict) and "error" in data:
                            output.error = str(data. get("error"))
                            output.success = False
                            break
                        
                        # æå–ç”Ÿæˆçš„æ–‡æœ¬
                        text_piece = ""
                        if isinstance(data, dict):
                            text_piece = data.get("text", "")
                        
                        if text_piece:
                            timestamp = time.perf_counter()
                            if ttft == 0.0:
                                ttft = timestamp - st
                                output.ttft = ttft
                            else:
                                output.itl. append(timestamp - most_recent_timestamp)
                            
                            most_recent_timestamp = timestamp
                            generated_text += text_piece
                    
                    output.generated_text = generated_text
                    output.success = True
                    output.latency = time.perf_counter() - st
                    output.output_len = len(generated_text. split())
                else:
                    output.error = f"HTTP {response.status}: {response.reason}"
                    output.success = False
                    
        except Exception as e:
            output.success = False
            output.error = f"Request failed: {str(e)}"
            exc_info = sys.exc_info()
            output.error += "\n" + "". join(traceback.format_exception(*exc_info))
        
        if pbar:
            pbar. update(1)
        
        return output


def calculate_metrics(
    outputs: List[RequestFuncOutput],
    scenario: str,
    total_time:  float
) -> BenchmarkMetrics: 
    """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
    successful = [o for o in outputs if o. success]
    num_completed = len(successful)
    num_failed = len(outputs) - num_completed
    
    if num_completed == 0:
        raise ValueError("æ‰€æœ‰è¯·æ±‚éƒ½å¤±è´¥äº†ï¼")
    
    # å»¶è¿Ÿç»Ÿè®¡
    latencies_ms = [o.latency * 1000 for o in successful]
    latencies_ms.sort()
    
    mean_latency = sum(latencies_ms) / len(latencies_ms)
    median_latency = latencies_ms[len(latencies_ms) // 2]
    p95_idx = int(len(latencies_ms) * 0.95)
    p95_latency = latencies_ms[p95_idx] if p95_idx < len(latencies_ms) else latencies_ms[-1]
    p99_idx = int(len(latencies_ms) * 0.99)
    p99_latency = latencies_ms[p99_idx] if p99_idx < len(latencies_ms) else latencies_ms[-1]
    
    # TTFT ç»Ÿè®¡
    ttfts_ms = [o.ttft * 1000 for o in successful if o.ttft > 0]
    mean_ttft = sum(ttfts_ms) / len(ttfts_ms) if ttfts_ms else 0
    ttfts_ms.sort()
    median_ttft = ttfts_ms[len(ttfts_ms) // 2] if ttfts_ms else 0
    
    # Token ç»Ÿè®¡
    total_input = sum(o.prompt_len for o in successful)
    total_output = sum(o.output_len for o in successful)
    
    # ååé‡
    throughput_rps = num_completed / total_time
    output_throughput_tps = total_output / total_time
    
    return BenchmarkMetrics(
        scenario=scenario,
        num_requests=len(outputs),
        completed=num_completed,
        failed=num_failed,
        total_time_s=total_time,
        mean_latency_ms=mean_latency,
        median_latency_ms=median_latency,
        p95_latency_ms=p95_latency,
        p99_latency_ms=p99_latency,
        mean_ttft_ms=mean_ttft,
        median_ttft_ms=median_ttft,
        total_input_tokens=total_input,
        total_output_tokens=total_output,
        throughput_rps=throughput_rps,
        output_throughput_tps=output_throughput_tps
    )


async def run_benchmark_scenario(
    api_url: str,
    base_model_id: str,
    requests: List[Tuple[str, int, int]],
    lora_assignment: str,  # "same" or "random"
    disable_tqdm: bool = False
) -> BenchmarkMetrics:
    """
    è¿è¡Œå•ä¸ª benchmark åœºæ™¯
    
    Args:
        api_url: SGLang æœåŠ¡å™¨ API URL
        base_model_id:  åŸºç¡€æ¨¡å‹ ID
        requests: æµ‹è¯•è¯·æ±‚åˆ—è¡¨
        lora_assignment: LoRA åˆ†é…ç­–ç•¥ ("same" æˆ– "random")
        disable_tqdm:  æ˜¯å¦ç¦ç”¨è¿›åº¦æ¡
    """
    scenario_name = f"{lora_assignment.upper()}_LORA"
    print(f"\n{'='*70}")
    print(f"åœºæ™¯: {scenario_name}")
    print(f"{'='*70}")
    
    if lora_assignment == "same": 
        print("æ‰€æœ‰ 8 ä¸ªè¯·æ±‚éƒ½ä½¿ç”¨ lora0")
    else:
        print(f"8 ä¸ªè¯·æ±‚éšæœºä½¿ç”¨ lora0 åˆ° lora{NUM_LORAS-1}")
    
    # å‡†å¤‡è¯·æ±‚
    tasks = []
    pbar = None if disable_tqdm else tqdm(total=len(requests), desc=scenario_name)
    
    start_time = time.perf_counter()
    
    for idx, (prompt, prompt_len, output_len) in enumerate(requests):
        # ç¡®å®šä½¿ç”¨å“ªä¸ª LoRA
        if lora_assignment == "same":
            lora_name = "lora0"
        else:   # random
            lora_id = random.randint(0, NUM_LORAS - 1)
            lora_name = f"lora{lora_id}"
        
        request_input = RequestFuncInput(
            model=base_model_id,
            prompt=prompt,
            api_url=api_url,
            prompt_len=prompt_len,
            output_len=output_len,
            lora_name=lora_name,
            image_data=None,
            extra_request_body={}
        )
        
        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        task = asyncio.create_task(
            async_request_sglang(
                request_func_input=request_input,
                pbar=pbar
            )
        )
        tasks.append(task)
    
    # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
    outputs = await asyncio.gather(*tasks)
    
    total_time = time.perf_counter() - start_time
    
    if pbar:
        pbar.close()
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = calculate_metrics(outputs, scenario_name, total_time)
    
    # æ‰“å°ç»“æœ
    print_metrics(metrics)
    
    return metrics


def print_metrics(metrics: BenchmarkMetrics):
    """æ‰“å°æ€§èƒ½æŒ‡æ ‡"""
    print(f"\n{'-'*70}")
    print(f"åœºæ™¯: {metrics.scenario}")
    print(f"{'-'*70}")
    print(f"æ€»è¯·æ±‚æ•°:         {metrics.num_requests}")
    print(f"æˆåŠŸ:             {metrics.completed}")
    print(f"å¤±è´¥:            {metrics.failed}")
    print(f"æ€»è€—æ—¶:          {metrics.total_time_s:.2f} s")
    print(f"\nå»¶è¿Ÿç»Ÿè®¡ (ms):")
    print(f"  å¹³å‡å»¶è¿Ÿ:      {metrics.mean_latency_ms:.2f}")
    print(f"  ä¸­ä½å»¶è¿Ÿ:      {metrics.median_latency_ms:.2f}")
    print(f"  P95 å»¶è¿Ÿ:      {metrics.p95_latency_ms:.2f}")
    print(f"  P99 å»¶è¿Ÿ:      {metrics.p99_latency_ms:.2f}")
    print(f"\nTTFT ç»Ÿè®¡ (ms):")
    print(f"  å¹³å‡ TTFT:     {metrics.mean_ttft_ms:.2f}")
    print(f"  ä¸­ä½ TTFT:     {metrics.median_ttft_ms:.2f}")
    print(f"\nToken ç»Ÿè®¡:")
    print(f"  æ€»è¾“å…¥ tokens:   {metrics.total_input_tokens}")
    print(f"  æ€»è¾“å‡º tokens:   {metrics.total_output_tokens}")
    print(f"\nååé‡:")
    print(f"  è¯·æ±‚ååé‡:    {metrics.throughput_rps:.2f} req/s")
    print(f"  Token ååé‡:  {metrics.output_throughput_tps:.2f} tok/s")
    print(f"{'='*70}")


def compare_results(
    same_lora_metrics: BenchmarkMetrics,
    random_lora_metrics:  BenchmarkMetrics
):
    """å¯¹æ¯”ä¸¤ä¸ªåœºæ™¯çš„ç»“æœ"""
    print(f"\n{'='*70}")
    print("æ€§èƒ½å¯¹æ¯”åˆ†æ")
    print(f"{'='*70}")
    
    # å»¶è¿Ÿå¯¹æ¯”
    latency_diff_pct = (
        (random_lora_metrics.mean_latency_ms - same_lora_metrics.mean_latency_ms)
        / same_lora_metrics.mean_latency_ms * 100
    )
    
    print(f"\nå¹³å‡å»¶è¿Ÿ:")
    print(f"  åŒä¸€LoRA (lora0):     {same_lora_metrics. mean_latency_ms:.2f} ms")
    print(f"  éšæœºLoRA (æ··åˆ):      {random_lora_metrics. mean_latency_ms:.2f} ms")
    print(f"  å·®å¼‚:                  {latency_diff_pct:+.2f}%")
    
    # TTFT å¯¹æ¯”
    ttft_diff_pct = (
        (random_lora_metrics.mean_ttft_ms - same_lora_metrics.mean_ttft_ms)
        / same_lora_metrics.mean_ttft_ms * 100
    )
    
    print(f"\nå¹³å‡ TTFT:")
    print(f"  åŒä¸€LoRA (lora0):     {same_lora_metrics. mean_ttft_ms:.2f} ms")
    print(f"  éšæœºLoRA (æ··åˆ):      {random_lora_metrics.mean_ttft_ms:.2f} ms")
    print(f"  å·®å¼‚:                 {ttft_diff_pct:+.2f}%")
    
    # ååé‡å¯¹æ¯”
    throughput_diff_pct = (
        (same_lora_metrics.throughput_rps - random_lora_metrics.throughput_rps)
        / same_lora_metrics.throughput_rps * 100
    )
    
    print(f"\nè¯·æ±‚ååé‡:")
    print(f"  åŒä¸€LoRA (lora0):     {same_lora_metrics.throughput_rps:.2f} req/s")
    print(f"  éšæœºLoRA (æ··åˆ):      {random_lora_metrics.throughput_rps:.2f} req/s")
    print(f"  ååé‡æŸå¤±:           {throughput_diff_pct:+.2f}%")
    
    # ç»“è®º
    print(f"\nğŸ’¡ å…³é”®å‘ç°:")
    if latency_diff_pct > 5:
        print(f"  â€¢ éšæœºLoRAæ¯”åŒä¸€LoRAå»¶è¿Ÿé«˜ {latency_diff_pct:.1f}%")
        print(f"  â€¢ è¿™éªŒè¯äº† LoRA å¤šæ ·æ€§å¸¦æ¥çš„æ€§èƒ½å¼€é”€")
        print(f"  â€¢ å»ºè®®ä½¿ç”¨ dLoRA ç­‰ä¼˜åŒ–ç­–ç•¥æ¥ç¼“è§£")
    elif latency_diff_pct > 1:
        print(f"  â€¢ éšæœºLoRAæœ‰è½»å¾®çš„æ€§èƒ½å¼€é”€ ({latency_diff_pct:.1f}%)")
        print(f"  â€¢ å½“å‰é…ç½®ä¸‹å¼€é”€å¯æ¥å—")
    else:
        print(f"  â€¢ ä¸¤ç§åœºæ™¯æ€§èƒ½ç›¸è¿‘ï¼Œå¼€é”€å¾ˆå°")
        print(f"  â€¢ SGMV kernel ä¼˜åŒ–è‰¯å¥½")
    
    print(f"{'='*70}")


async def main_async(args):
    """ä¸»æµ‹è¯•å‡½æ•°"""
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    
    # å‡†å¤‡æµ‹è¯•è¯·æ±‚
    print("å‡†å¤‡æµ‹è¯•è¯·æ±‚...")
    requests = prepare_test_requests(
        num_requests=args.num_requests,
        prompt_len=args.prompt_len,
        output_len=args.output_len
    )
    print(f"âœ“ å‡†å¤‡äº† {len(requests)} ä¸ªæµ‹è¯•è¯·æ±‚")
    
    # æœåŠ¡å™¨ URL
    api_url = f"http://{args.host}:{args. port}/generate"
    base_model_id = LORA_PATH["base"]
    
    print(f"\næœåŠ¡å™¨é…ç½®:")
    print(f"  URL: {api_url}")
    print(f"  æ¨¡å‹: {base_model_id}")
    print(f"  LoRA æ•°é‡: {NUM_LORAS}")
    print(f"  LoRA åç«¯: {args.lora_backend}")
    
    # æµ‹è¯•æœåŠ¡å™¨è¿æ¥
    print(f"\næµ‹è¯•æœåŠ¡å™¨è¿æ¥...")
    try:
        async with _create_bench_client_session() as session:
            async with session.get(f"http://{args.host}:{args.port}/health") as resp:
                if resp.status == 200:
                    print("âœ“ æœåŠ¡å™¨è¿æ¥æ­£å¸¸")
                else:
                    print(f"âš ï¸  æœåŠ¡å™¨è¿”å›çŠ¶æ€ç : {resp.status}")
    except Exception as e: 
        print(f"âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨: {e}")
        print(f"è¯·ç¡®ä¿æœåŠ¡å™¨å·²å¯åŠ¨:  http://{args.host}:{args.port}")
        return 1
    
    # åœºæ™¯ 1: æ‰€æœ‰è¯·æ±‚ä½¿ç”¨ lora0
    print(f"\n{'='*70}")
    print("å¼€å§‹æµ‹è¯•åœºæ™¯ 1: æ‰€æœ‰è¯·æ±‚ä½¿ç”¨ lora0")
    print(f"{'='*70}")
    
    same_lora_metrics = await run_benchmark_scenario(
        api_url=api_url,
        base_model_id=base_model_id,
        requests=requests,
        lora_assignment="same",
        disable_tqdm=args.disable_tqdm
    )
    
    # ç­‰å¾…ä¸€ä¸‹å†å¼€å§‹ä¸‹ä¸€ä¸ªåœºæ™¯
    print("\nç­‰å¾… 2 ç§’åå¼€å§‹ä¸‹ä¸€ä¸ªåœºæ™¯...")
    await asyncio.sleep(2)
    
    # åœºæ™¯ 2: éšæœºä½¿ç”¨ä¸åŒ LoRA
    print(f"\n{'='*70}")
    print("å¼€å§‹æµ‹è¯•åœºæ™¯ 2: éšæœºä½¿ç”¨ä¸åŒ LoRA")
    print(f"{'='*70}")
    
    random_lora_metrics = await run_benchmark_scenario(
        api_url=api_url,
        base_model_id=base_model_id,
        requests=requests,
        lora_assignment="random",
        disable_tqdm=args.disable_tqdm
    )
    
    # å¯¹æ¯”ç»“æœ
    compare_results(same_lora_metrics, random_lora_metrics)
    
    # ä¿å­˜ç»“æœ
    if args.output_file:
        results = {
            "config": {
                "num_requests": args.num_requests,
                "prompt_len":  args.prompt_len,
                "output_len": args.output_len,
                "lora_backend": args.lora_backend,
                "num_loras": NUM_LORAS,
            },
            "same_lora":  {
                "scenario": same_lora_metrics.scenario,
                "mean_latency_ms": same_lora_metrics.mean_latency_ms,
                "median_latency_ms": same_lora_metrics.median_latency_ms,
                "mean_ttft_ms": same_lora_metrics.mean_ttft_ms,
                "throughput_rps": same_lora_metrics.throughput_rps,
            },
            "random_lora": {
                "scenario": random_lora_metrics.scenario,
                "mean_latency_ms": random_lora_metrics.mean_latency_ms,
                "median_latency_ms": random_lora_metrics.median_latency_ms,
                "mean_ttft_ms": random_lora_metrics.mean_ttft_ms,
                "throughput_rps": random_lora_metrics. throughput_rps,
            }
        }
        
        with open(args.output_file, "w") as f:
            json.dump(results, f, indent=2)
        
        print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {args.output_file}")
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")
    return 0


def main():
    parser = argparse.ArgumentParser(
        description="ç®€åŒ–ç‰ˆ LoRA Benchmark - æµ‹è¯•åŒä¸€LoRA vs éšæœºLoRA"
    )
    
    # æœåŠ¡å™¨é…ç½®
    parser. add_argument("--host", type=str, default="127.0.0.1",
                       help="SGLang æœåŠ¡å™¨åœ°å€")
    parser.add_argument("--port", type=int, default=30001,
                       help="SGLang æœåŠ¡å™¨ç«¯å£")
    
    # æµ‹è¯•é…ç½®
    parser.add_argument("--num-requests", type=int, default=8,
                       help="æ¯ä¸ªåœºæ™¯çš„è¯·æ±‚æ•°é‡")
    parser.add_argument("--prompt-len", type=int, default=256,
                       help="è¾“å…¥ prompt é•¿åº¦")
    parser.add_argument("--output-len", type=int, default=128,
                       help="è¾“å‡º token é•¿åº¦")
    
    # LoRA é…ç½®
    parser.add_argument("--lora-backend", type=str, default="csgmv",
                       choices=["triton", "csgmv"],
                       help="LoRA åç«¯")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42,
                       help="éšæœºç§å­")
    parser.add_argument("--output-file", type=str, default=None,
                       help="ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶")
    parser.add_argument("--disable-tqdm", action="store_true",
                       help="ç¦ç”¨è¿›åº¦æ¡")
    
    args = parser.parse_args()
    
    try:
        exit_code = asyncio.run(main_async(args))
        return exit_code
    except KeyboardInterrupt: 
        print("\n\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())