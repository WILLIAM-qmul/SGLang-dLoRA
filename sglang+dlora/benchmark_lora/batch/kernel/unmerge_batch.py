"""
æœªåˆå¹¶æ¨ç†æ€§èƒ½å¯¹æ¯”æµ‹è¯•ï¼šåŒä¸€LoRA vs ä¸åŒLoRA
æµ‹è¯•åœºæ™¯ï¼šWX + BAXï¼ˆå®Œæ•´çš„æœªåˆå¹¶æ¨ç†æµç¨‹ï¼‰
"""

import torch
import time
from typing import List, Dict
from dataclasses import dataclass

from sglang.srt. lora. backend. chunked_backend import ChunkedSgmvLoRABackend
from sglang.srt.lora.backend.triton_backend import TritonLoRABackend
from sglang.srt.lora.backend.base_backend import BaseLoRABackend


@dataclass
class UnmergedConfig:
    """æœªåˆå¹¶æ¨ç†æµ‹è¯•é…ç½®"""
    batch_size: int = 16
    seq_len: int = 128
    hidden_dim: int = 1024
    lora_rank: int = 32
    num_loras: int = 4
    test_iterations: int = 1000
    device: str = "cuda:1"
    max_chunk_size:  int = 8


class MinimalServerArgs: 
    def __init__(self, max_lora_chunk_size: int = 32):
        self.max_lora_chunk_size = max_lora_chunk_size
        self.model_path = "/tmp/dummy"


class SimpleForwardMode:
    def is_extend(self) -> bool:
        return True


class SimpleForwardBatch:
    def __init__(self, batch_size: int, seq_len: int):
        self.batch_size = batch_size
        self.extend_seq_lens_cpu = [seq_len] * batch_size
        self.forward_mode = SimpleForwardMode()
        self.extend_num_tokens = batch_size * seq_len


class UnmergedLoRAInferenceTester:
    """æœªåˆå¹¶LoRAæ¨ç†æµ‹è¯•å™¨"""
    
    def __init__(self, config:  UnmergedConfig):
        self.config = config
        self.device = torch.device(config.device)
        torch.cuda.set_device(self.device)
        
        # åˆå§‹åŒ–åç«¯
        server_args = MinimalServerArgs(config. max_chunk_size)
        self.backend = ChunkedSgmvLoRABackend(
            max_loras_per_batch=config.num_loras,
            device=self.device,
            server_args=server_args
        )
        # self.backend = TritonLoRABackend(
        #     max_loras_per_batch=self.config.num_loras,
        #     device=self.device
        # )
        
        # åˆ›å»ºæƒé‡
        self. base_weight = self._create_base_weight()
        self.lora_weights_a, self.lora_weights_b = self._create_lora_weights()
        
        print("âœ“ Unmerged LoRA Inference Tester initialized")
        print(f"  Base weight shape: {self.base_weight.shape}")
        print(f"  LoRA A shape: {self.lora_weights_a.shape}")
        print(f"  LoRA B shape: {self.lora_weights_b.shape}")
    
    def _create_base_weight(self) -> torch.Tensor:
        """åˆ›å»ºåŸºç¡€æ¨¡å‹æƒé‡ W"""
        return torch.randn(
            self.config. hidden_dim,
            self.config. hidden_dim,
            dtype=torch.float16,
            device=self.device
        )
    
    def _create_lora_weights(self) -> tuple:
        """åˆ›å»º LoRA æƒé‡ A å’Œ B"""
        weights_a = torch. randn(
            self.config.num_loras,
            self.config. lora_rank,
            self.config. hidden_dim,
            dtype=torch. float16,
            device=self.device
        )
        
        weights_b = torch.randn(
            self.config.num_loras,
            self. config.hidden_dim,
            self.config.lora_rank,
            dtype=torch.float16,
            device=self.device
        )
        
        return weights_a, weights_b
    
    def _create_input(self) -> torch.Tensor:
        """åˆ›å»ºè¾“å…¥æ•°æ®"""
        total_tokens = self.config. batch_size * self.config.seq_len
        return torch.randn(
            total_tokens,
            self.config.hidden_dim,
            dtype=torch.float16,
            device=self.device
        )
    
    def _prepare_backend(self, weight_indices: List[int]):
        """å‡†å¤‡åç«¯æ‰¹æ¬¡ä¿¡æ¯"""
        forward_batch = SimpleForwardBatch(self.config.batch_size, self.config.seq_len)
        lora_ranks = [self.config.lora_rank] * self.config.num_loras
        scalings = [1.0] * self.config.num_loras
        
        self.backend.prepare_lora_batch(
            forward_batch=forward_batch,
            weight_indices=weight_indices,
            lora_ranks=lora_ranks,
            scalings=scalings,
            batch_info=None
        )
    
    def unmerged_inference(self, x: torch.Tensor) -> torch.Tensor:
        """
        å®Œæ•´çš„æœªåˆå¹¶æ¨ç†ï¼šWX + BAX
        è¿™æ˜¯ SGLang ä¸­å®é™…ä½¿ç”¨çš„æ–¹å¼
        """
        # 1. åŸºç¡€æ¨¡å‹è®¡ç®—ï¼šWX
        base_output = torch.mm(x, self. base_weight. T)  # (total_tokens, hidden_dim)
        
        # 2. LoRA è®¡ç®—ï¼šBAX (ä½¿ç”¨ SGMV kernel)
        # LoRA A (shrink): x @ A^T -> (total_tokens, lora_rank)
        lora_a_output = self.backend.run_lora_a_sgemm(x, self.lora_weights_a)
        
        # LoRA B (expand): lora_a_output @ B^T -> (total_tokens, hidden_dim)
        output_offset = torch.tensor([0, self.config.hidden_dim], dtype=torch.int32, device=x.device)
        lora_output = self.backend. run_lora_b_sgemm(
            x=lora_a_output,
            weights=self.lora_weights_b,
            output_offset=output_offset,
            base_output=base_output  # ä¼šè‡ªåŠ¨åŠ åˆ° base_output ä¸Šï¼šWX + BAX
        )
        
        return lora_output
    
    def benchmark_unmerged_inference(self, weight_indices: List[int], scenario_name: str) -> Dict[str, float]:
        """æµ‹è¯•æœªåˆå¹¶æ¨ç†æ€§èƒ½"""
        print(f"\nğŸ”„ Testing {scenario_name}")
        print(f"   Weight indices: {weight_indices[: 8]}{'...' if len(weight_indices) > 8 else ''}")
        
        # å‡†å¤‡åç«¯
        self._prepare_backend(weight_indices)
        
        # åˆ›å»ºè¾“å…¥
        x = self._create_input()
        
        # Warmup
        try:
            _ = self.unmerged_inference(x)
            torch.cuda.synchronize()
            print("   âœ“ Warmup completed")
        except Exception as e:
            print(f"   âŒ Warmup failed: {e}")
            raise
        
        # æ€§èƒ½æµ‹è¯•
        torch.cuda.synchronize()
        start_time = time. perf_counter()
        
        for _ in range(self. config.test_iterations):
            _ = self.unmerged_inference(x)
        
        torch.cuda.synchronize()
        end_time = time.perf_counter()
        
        # è®¡ç®—æŒ‡æ ‡
        total_time_ms = (end_time - start_time) * 1000
        avg_time_ms = total_time_ms / self.config. test_iterations
        avg_time_us = avg_time_ms * 1000
        throughput = (self.config.batch_size * self. config.test_iterations) / (total_time_ms / 1000)
        
        print(f"   â±ï¸  Average time: {avg_time_us:.1f} us")
        print(f"   ğŸš€ Throughput: {throughput:.1f} requests/sec")
        
        return {
            'scenario': scenario_name,
            'avg_time_us': avg_time_us,
            'throughput_rps': throughput,
            'weight_indices': weight_indices.copy()
        }
    
    def run_comparison_test(self):
        """è¿è¡Œå¯¹æ¯”æµ‹è¯•ï¼šåŒä¸€LoRA vs ä¸åŒLoRA"""
        print("\n" + "ğŸš€ æœªåˆå¹¶æ¨ç†æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
        print("=" * 70)
        print("æµ‹è¯•å†…å®¹ï¼šå®Œæ•´çš„æœªåˆå¹¶æ¨ç† WX + BAX")
        print("å¯¹æ¯”åœºæ™¯ï¼šåŒä¸€LoRA vs ä¸åŒLoRA åœ¨ SGMV kernel ä¸­çš„æ€§èƒ½")
        print("=" * 70)
        
        # åœºæ™¯ 1: æ‰€æœ‰è¯·æ±‚ä½¿ç”¨åŒä¸€ä¸ª LoRA (lora_0)
        print(f"\nğŸ“Š åœºæ™¯ 1: æ‰€æœ‰ {self.config.batch_size} ä¸ªè¯·æ±‚éƒ½ä½¿ç”¨ LoRA_0")
        weight_indices_same = [0] * self.config.batch_size
        result_same = self. benchmark_unmerged_inference(weight_indices_same, "Same LoRA (all lora_0)")
        
        # åœºæ™¯ 2: æ¯ä¸ªè¯·æ±‚ä½¿ç”¨ä¸åŒçš„ LoRA
        print(f"\nğŸ“Š åœºæ™¯ 2: æ¯ä¸ªè¯·æ±‚ä½¿ç”¨ä¸åŒçš„ LoRA (å¾ªç¯ä½¿ç”¨ LoRA_0 åˆ° LoRA_{self.config.num_loras-1})")
        weight_indices_diff = [i % self. config.num_loras for i in range(self.config. batch_size)]
        result_diff = self.benchmark_unmerged_inference(weight_indices_diff, "Different LoRAs (mixed)")
        
        # è¯¦ç»†åˆ†æ
        print("\n" + "=" * 70)
        print("ğŸ“ˆ æ€§èƒ½åˆ†æç»“æœ")
        print("=" * 70)
        
        # åŸºæœ¬æŒ‡æ ‡
        print(f"åŒä¸€LoRA (lora_0):")
        print(f"  å¹³å‡æ—¶é—´:      {result_same['avg_time_us']:.1f} ms")
        print(f"  ååé‡:        {result_same['throughput_rps']:.1f} requests/sec")
        
        print(f"\nä¸åŒLoRA (æ··åˆ):")
        print(f"  å¹³å‡æ—¶é—´:      {result_diff['avg_time_us']:.1f} ms") 
        print(f"  ååé‡:       {result_diff['throughput_rps']:.1f} requests/sec")
        
        # æ€§èƒ½å·®å¼‚åˆ†æ
        if result_same['avg_time_us'] > 0:
            overhead_pct = (result_diff['avg_time_us'] - result_same['avg_time_us']) / result_same['avg_time_us'] * 100
            throughput_loss = (result_same['throughput_rps'] - result_diff['throughput_rps']) / result_same['throughput_rps'] * 100
            
            print(f"\nğŸ’¡ å…³é”®å‘ç°:")
            print(f"  æ—¶é—´å¼€é”€:     {overhead_pct:+.2f}%")
            print(f"  ååé‡æŸå¤±:   {throughput_loss:+.2f}%")
            
            if overhead_pct > 5:
                print(f"  ğŸ” ä¸åŒLoRAæ¯”åŒä¸€LoRAæ…¢ {overhead_pct:.1f}%")
                print(f"     è¿™éªŒè¯äº†dLoRAè®ºæ–‡ä¸­å…³äºSGMV kernelæ€§èƒ½å·®å¼‚çš„è§‚ç‚¹")
                print(f"     åŸå› ï¼šä¸åŒLoRAéœ€è¦è®¿é—®ä¸åŒçš„æƒé‡ï¼Œç¼“å­˜å±€éƒ¨æ€§è¾ƒå·®")
            elif overhead_pct > 1:
                print(f"  âœ… æ€§èƒ½å·®å¼‚è¾ƒå° ({overhead_pct:.1f}%)ï¼ŒSGMV kernelä¼˜åŒ–è‰¯å¥½")
            else:
                print(f"  ğŸ¤” æ€§èƒ½å·®å¼‚å¾ˆå°ï¼Œå¯èƒ½å—åˆ°å…¶ä»–å› ç´ å½±å“")
        
        # è¯¦ç»†çš„æƒé‡ç´¢å¼•ä¿¡æ¯
        print(f"\nğŸ” è¯¦ç»†ä¿¡æ¯:")
        print(f"  æ‰¹æ¬¡å¤§å°:     {self.config. batch_size}")
        print(f"  åºåˆ—é•¿åº¦:     {self.config. seq_len}")
        print(f"  æ€»tokenæ•°:    {self.config. batch_size * self. config.seq_len}")
        print(f"  LoRAæ•°é‡:      {self.config.num_loras}")
        print(f"  LoRAç»´åº¦:     {self.config.lora_rank}")
        
        unique_loras_diff = len(set(weight_indices_diff))
        print(f"  åœºæ™¯1ä½¿ç”¨LoRA: 1ä¸ª (lora_0)")
        print(f"  åœºæ™¯2ä½¿ç”¨LoRA:  {unique_loras_diff}ä¸ª (lora_0 åˆ° lora_{unique_loras_diff-1})")
        
        print("=" * 70)
        
        return {
            'same_lora': result_same,
            'different_lora': result_diff,
            'overhead_pct': overhead_pct if result_same['avg_time_us'] > 0 else 0,
            'throughput_loss_pct': throughput_loss if result_same['avg_time_us'] > 0 else 0
        }
    
    def run_batch_size_analysis(self):
        """åˆ†æä¸åŒbatch sizeä¸‹çš„æ€§èƒ½å·®å¼‚"""
        print("\nğŸ”¬ Batch Size å½±å“åˆ†æ")
        print("=" * 50)
        
        original_batch_size = self.config.batch_size
        batch_sizes = [4, 8, 16, 32, 64]
        results = []
        
        for bs in batch_sizes: 
            if bs > 64:  # é¿å…å†…å­˜è¶…é™
                continue
                
            print(f"\næµ‹è¯• Batch Size: {bs}")
            self.config.batch_size = bs
            
            # åŒä¸€LoRA
            weight_indices_same = [0] * bs
            result_same = self.benchmark_unmerged_inference(weight_indices_same, f"Same-BS{bs}")
            
            # ä¸åŒLoRA  
            weight_indices_diff = [i % self.config.num_loras for i in range(bs)]
            result_diff = self.benchmark_unmerged_inference(weight_indices_diff, f"Diff-BS{bs}")

            overhead = (result_diff['avg_time_us'] - result_same['avg_time_us']) / result_same['avg_time_us'] * 100

            results.append({
                'batch_size': bs,
                'same_time': result_same['avg_time_us'],
                'diff_time': result_diff['avg_time_us'],
                'overhead_pct': overhead
            })

            print(f"  åŒä¸€LoRA:  {result_same['avg_time_us']:. 1f}ms")
            print(f"  ä¸åŒLoRA: {result_diff['avg_time_us']:.1f}ms")
            print(f"  å¼€é”€: {overhead: +.1f}%")
        
        # æ¢å¤åŸå§‹é…ç½®
        self. config.batch_size = original_batch_size
        
        print(f"\nğŸ“Š Batch Size åˆ†ææ€»ç»“:")
        for r in results:
            print(f"  BS={r['batch_size']:2d}: å¼€é”€={r['overhead_pct']: +5.1f}%")
        
        return results


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    
    # é…ç½®
    config = UnmergedConfig(
        batch_size=8,
        seq_len=512,
        hidden_dim=4096,
        lora_rank=64,
        num_loras=8,
        test_iterations=100000,  # å¢åŠ è¿­ä»£æ¬¡æ•°è·å¾—æ›´ç¨³å®šçš„ç»“æœ
        device="cuda:1",
        max_chunk_size=16
    )
    
    print("ğŸ”¬ SGLang æœªåˆå¹¶æ¨ç†æ€§èƒ½æµ‹è¯•")
    print("=" * 70)
    print("ç›®æ ‡ï¼šæµ‹è¯• SGMV kernel åœ¨å¤„ç†åŒä¸€LoRA vs ä¸åŒLoRAæ—¶çš„æ€§èƒ½å·®å¼‚")
    print("æ–¹æ³•ï¼šå®Œæ•´çš„æœªåˆå¹¶æ¨ç† (WX + BAX)")
    print("=" * 70)
    
    try:
        # åˆ›å»ºæµ‹è¯•å™¨
        tester = UnmergedLoRAInferenceTester(config)
        
        # è¿è¡Œä¸»è¦å¯¹æ¯”æµ‹è¯•
        comparison_results = tester.run_comparison_test()
        
        # è¿è¡Œbatch sizeåˆ†æ
        # batch_analysis = tester.run_batch_size_analysis()
        
        print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")
        print("\nğŸ’¡ ä¸»è¦ç»“è®º:")
        overhead = comparison_results['overhead_pct']
        if overhead > 10:
            print(f"1. ä¸åŒLoRAæœ‰æ˜¾è‘—çš„æ€§èƒ½å¼€é”€ ({overhead:.1f}%)")
            print("2. è¿™æ”¯æŒäº†dLoRAè®ºæ–‡ä¸­åŠ¨æ€æ‰¹å¤„ç†çš„å¿…è¦æ€§")
        elif overhead > 3:
            print(f"1. ä¸åŒLoRAæœ‰ä¸­ç­‰çš„æ€§èƒ½å¼€é”€ ({overhead:.1f}%)")
            print("2. SGMV kernelä¼˜åŒ–è¾ƒå¥½ï¼Œä½†ä»æœ‰ä¼˜åŒ–ç©ºé—´")  
        else: 
            print(f"1. ä¸åŒLoRAçš„æ€§èƒ½å¼€é”€å¾ˆå° ({overhead:.1f}%)")
            print("2. å½“å‰é…ç½®ä¸‹SGMV kernelè¡¨ç°è‰¯å¥½")
        
        print("3. å¯ä»¥é€šè¿‡è°ƒæ•´batch sizeå’ŒLoRAé…ç½®æ¥è¿›ä¸€æ­¥ä¼˜åŒ–")
        
    except Exception as e: 
        print(f"\nâŒ æµ‹è¯•å¤±è´¥:  {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)