"""
LoRA SGMV Kernel åŸºç¡€æ€§èƒ½æµ‹è¯•
æµ‹è¯•åŒä¸€ LoRA vs ä¸åŒ LoRA çš„æ‰¹å¤„ç†æ€§èƒ½å·®å¼‚ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
"""

import torch
import time
from typing import List, Dict
from dataclasses import dataclass

from sglang.srt.lora. backend.chunked_backend import ChunkedSgmvLoRABackend


@dataclass
class SimpleConfig:
    """ç®€åŒ–çš„æµ‹è¯•é…ç½® - å‡å°‘å†…å­˜ä½¿ç”¨"""
    batch_size: int = 16          # å‡å° batch size
    seq_len: int = 128            # å‡å°åºåˆ—é•¿åº¦  
    hidden_dim:  int = 1024       # å‡å°éšè—å±‚ç»´åº¦
    lora_rank: int = 32          # å‡å° LoRA rank
    num_loras: int = 4
    test_iterations: int = 10    # å‡å°‘æµ‹è¯•æ¬¡æ•°
    device: str = "cuda:1"
    max_chunk_size: int = 32     # å‡å° chunk size


class MinimalServerArgs:
    """æœ€å°åŒ–çš„ ServerArgs"""
    def __init__(self, max_lora_chunk_size: int = 32):
        self.max_lora_chunk_size = max_lora_chunk_size
        self.model_path = "/tmp/dummy"


class SimpleForwardMode:
    """ç®€åŒ–çš„ ForwardMode"""
    def is_extend(self) -> bool:
        return True


class SimpleForwardBatch:
    """ç®€åŒ–çš„ ForwardBatch"""
    def __init__(self, batch_size: int, seq_len: int):
        self.batch_size = batch_size
        self. extend_seq_lens_cpu = [seq_len] * batch_size
        self.forward_mode = SimpleForwardMode()
        self.extend_num_tokens = batch_size * seq_len


def create_lora_weights(config: SimpleConfig, device: torch.device):
    """åˆ›å»º LoRA æƒé‡"""
    weights_a = torch.randn(
        config.num_loras, 
        config.lora_rank, 
        config.hidden_dim,
        dtype=torch.float16, 
        device=device
    )
    
    weights_b = torch.randn(
        config.num_loras, 
        config.hidden_dim, 
        config.lora_rank,
        dtype=torch.float16, 
        device=device
    )
    
    return weights_a, weights_b


def create_input(config: SimpleConfig, device: torch.device):
    """åˆ›å»ºè¾“å…¥æ•°æ®"""
    total_tokens = config.batch_size * config.seq_len
    return torch.randn(
        total_tokens, 
        config. hidden_dim,
        dtype=torch. float16, 
        device=device
    )


def prepare_backend(backend, config: SimpleConfig, weight_indices: List[int]):
    """å‡†å¤‡åç«¯æ‰¹æ¬¡ä¿¡æ¯"""
    forward_batch = SimpleForwardBatch(config.batch_size, config.seq_len)
    lora_ranks = [config.lora_rank] * config.num_loras
    scalings = [1.0] * config.num_loras
    
    backend.prepare_lora_batch(
        forward_batch=forward_batch,
        weight_indices=weight_indices,
        lora_ranks=lora_ranks,
        scalings=scalings,
        batch_info=None
    )


def run_lora_forward(backend, weights_a, weights_b, x, config: SimpleConfig):
    """è¿è¡Œ LoRA å‰å‘ä¼ æ’­"""
    # LoRA A (shrink)
    lora_a_output = backend.run_lora_a_sgemm(x, weights_a)
    
    # LoRA B (expand)  
    output_offset = torch.tensor([0, config.hidden_dim], dtype=torch. int32, device=x. device)
    output = backend.run_lora_b_sgemm(
        x=lora_a_output,
        weights=weights_b, 
        output_offset=output_offset,
        base_output=None
    )
    
    return output


def benchmark_scenario(backend, weights_a, weights_b, config: SimpleConfig, 
                      weight_indices: List[int], scenario_name: str) -> float:
    """æµ‹è¯•å•ä¸ªåœºæ™¯"""
    print(f"\nğŸ”„ Testing {scenario_name}")
    print(f"   Weight indices: {weight_indices}")
    
    # å‡†å¤‡åç«¯
    prepare_backend(backend, config, weight_indices)
    
    # åˆ›å»ºè¾“å…¥
    x = create_input(config, backend.device)
    
    # Warmup (åªåš1æ¬¡)
    try:
        _ = run_lora_forward(backend, weights_a, weights_b, x, config)
        torch.cuda.synchronize()
        print("   âœ“ Warmup completed")
    except Exception as e: 
        print(f"   âŒ Warmup failed: {e}")
        raise
    
    # æ€§èƒ½æµ‹è¯•
    torch.cuda.synchronize()
    start_time = time. perf_counter()
    
    for _ in range(config. test_iterations):
        _ = run_lora_forward(backend, weights_a, weights_b, x, config)
    
    torch.cuda.synchronize()
    end_time = time.perf_counter()
    
    # è®¡ç®—å¹³å‡æ—¶é—´
    total_time_ms = (end_time - start_time) * 1000
    avg_time_ms = total_time_ms / config. test_iterations
    
    print(f"   â±ï¸  Average time: {avg_time_ms:.4f} ms")
    
    return avg_time_ms


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    
    # é…ç½® - ä½¿ç”¨æ›´å°çš„å‚æ•°ä»¥é¿å…å†…å­˜é—®é¢˜
    config = SimpleConfig(
        batch_size=16,
        seq_len=128, 
        hidden_dim=1024,
        lora_rank=32,
        num_loras=4,
        test_iterations=10000,
        device="cuda:1",
        max_chunk_size=8
    )
    
    print("ğŸš€ LoRA SGMV Kernel åŸºç¡€æ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    print(f"é…ç½®:")
    print(f"  Device: {config.device}")
    print(f"  Batch Size: {config.batch_size}")
    print(f"  Sequence Length: {config. seq_len}") 
    print(f"  Hidden Dimension: {config.hidden_dim}")
    print(f"  LoRA Rank: {config.lora_rank}")
    print(f"  Test Iterations: {config.test_iterations}")
    
    try:
        # åˆå§‹åŒ–è®¾å¤‡
        device = torch.device(config.device)
        torch.cuda.set_device(device)
        
        # åˆ›å»ºåç«¯
        server_args = MinimalServerArgs(config.max_chunk_size)
        backend = ChunkedSgmvLoRABackend(
            max_loras_per_batch=config.num_loras,
            device=device,
            server_args=server_args
        )
        print("âœ“ Backend initialized")
        
        # åˆ›å»º LoRA æƒé‡
        weights_a, weights_b = create_lora_weights(config, device)
        print(f"âœ“ LoRA weights created:  A{weights_a.shape}, B{weights_b.shape}")
        
        # æµ‹è¯•åœºæ™¯ 1: æ‰€æœ‰åºåˆ—ä½¿ç”¨ç›¸åŒçš„ LoRA (lora_0)
        weight_indices_same = [0] * config.batch_size
        time_same = benchmark_scenario(
            backend, weights_a, weights_b, config, 
            weight_indices_same, "Same LoRA (all use lora_0)"
        )
        
        # æµ‹è¯•åœºæ™¯ 2: æ¯ä¸ªåºåˆ—ä½¿ç”¨ä¸åŒçš„ LoRA  
        weight_indices_diff = [i % config.num_loras for i in range(config.batch_size)]
        time_diff = benchmark_scenario(
            backend, weights_a, weights_b, config,
            weight_indices_diff, "Different LoRAs (mixed)"
        )
        
        # ç»“æœå¯¹æ¯”
        print("\n" + "=" * 60)
        print("ğŸ“Š æµ‹è¯•ç»“æœ")
        print("=" * 60)
        print(f"Same LoRA time:       {time_same:.4f} ms")
        print(f"Different LoRA time: {time_diff:.4f} ms")
        
        if time_same > 0:
            overhead_pct = (time_diff - time_same) / time_same * 100
            speedup = time_same / time_diff
            
            print(f"Overhead:             {overhead_pct:+.2f}%")
            if overhead_pct > 0: 
                print(f"ğŸ’¡ Different LoRAs are {overhead_pct:.1f}% slower than same LoRA")
                print(f"   (éªŒè¯äº† dLoRA è®ºæ–‡ä¸­çš„é¢å¤–è®¡ç®—å¼€é”€)")
            else:
                print(f"ğŸ’¡ Different LoRAs are {abs(overhead_pct):.1f}% faster (unexpected!)")
        
        print("=" * 60)
        print("ğŸ‰ æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥:  {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__": 
    exit_code = main()
    exit(exit_code)