"""
LoRA SGMV Kernel Performance Benchmark
æµ‹è¯•åŒä¸€ LoRA vs ä¸åŒ LoRA çš„æ‰¹å¤„ç†æ€§èƒ½å·®å¼‚
"""

import torch
import time
import numpy as np
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import matplotlib.pyplot as plt
import sys
import os

from sglang.srt.lora.backend.chunked_backend import ChunkedSgmvLoRABackend
from sglang.srt.lora. utils import LoRABatchInfo  
from sglang.srt.server_args import ServerArgs


@dataclass
class BenchmarkConfig:
    """åŸºå‡†æµ‹è¯•é…ç½®"""
    batch_size: int = 16
    seq_len: int = 128
    hidden_dim: int = 4096
    lora_rank: int = 64
    num_loras: int = 4
    warmup_iterations: int = 10
    test_iterations: int = 100
    device: str = "cuda:0"
    max_chunk_size: int = 128


class MockServerArgs:
    """æ¨¡æ‹Ÿ ServerArgs ä»¥é¿å…å¤æ‚çš„ä¾èµ–"""
    
    def __init__(self, max_lora_chunk_size: int = 128):
        self.max_lora_chunk_size = max_lora_chunk_size
        
        # æ·»åŠ å…¶ä»–å¯èƒ½éœ€è¦çš„é»˜è®¤å±æ€§
        self. model_path = "/tmp/dummy_model"  # å¿…éœ€å‚æ•°
        self. dtype = "auto"
        self.tp_size = 1
        self.pp_size = 1
        self.device = "cuda"


class MockForwardMode:
    """æ¨¡æ‹Ÿ ForwardMode å¯¹è±¡"""
    
    def __init__(self, is_extend:  bool = True):
        self._is_extend = is_extend
    
    def is_extend(self) -> bool:
        return self._is_extend


class MockForwardBatch:
    """æ¨¡æ‹Ÿ ForwardBatch å¯¹è±¡"""
    
    def __init__(self, batch_size: int, seq_lens_cpu: List[int]):
        self.batch_size = batch_size
        self. extend_seq_lens_cpu = seq_lens_cpu
        self.forward_mode = MockForwardMode(is_extend=True)  # è®¾ç½®ä¸ºå±æ€§
        
        # è®¡ç®— extend_num_tokensï¼ˆæ‰©å±•æ¨¡å¼ä¸‹çš„ token æ€»æ•°ï¼‰
        self.extend_num_tokens = sum(seq_lens_cpu)


class LoRABenchmark:
    """LoRA æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self, config: BenchmarkConfig):
        self.config = config
        self.device = torch.device(config.device)
        
        # æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§
        if not torch.cuda.is_available():
            raise RuntimeError("CUDA is not available")
        if self.device.index is not None and self.device.index >= torch.cuda. device_count():
            raise RuntimeError(f"GPU {config.device} is not available")
        
        # è®¾ç½®è®¾å¤‡
        torch.cuda.set_device(self.device)
        
        # åˆå§‹åŒ–æ¨¡æ‹Ÿçš„ server_args
        server_args = MockServerArgs(max_lora_chunk_size=config.max_chunk_size)
        
        # åˆå§‹åŒ–åç«¯
        self. backend = ChunkedSgmvLoRABackend(
            max_loras_per_batch=config.num_loras,
            device=self.device,
            server_args=server_args
        )
        
        # åˆ›å»º LoRA æƒé‡
        self. weights_a, self.weights_b = self._create_lora_weights()
        
        print("âœ“ Benchmark initialized")
        print(f"  Device: {config.device}")
        print(f"  GPU Memory: {torch. cuda.get_device_properties(self.device).total_memory / 1e9:.1f} GB")
        print(f"  Batch Size:  {config.batch_size}")
        print(f"  Sequence Length: {config.seq_len}")
        print(f"  Hidden Dimension: {config.hidden_dim}")
        print(f"  LoRA Rank: {config.lora_rank}")
        print(f"  Number of LoRAs: {config.num_loras}")
    
    def _create_lora_weights(self) -> Tuple[torch. Tensor, torch. Tensor]:
        """åˆ›å»º LoRA æƒé‡ (A å’Œ B çŸ©é˜µ)"""
        # LoRA A: (num_loras, lora_rank, hidden_dim) - ç”¨äº shrink æ“ä½œ
        weights_a = torch. randn(
            self.config. num_loras,
            self.config.lora_rank,
            self. config.hidden_dim,
            dtype=torch.float16,
            device=self.device
        )
        
        # LoRA B:  (num_loras, hidden_dim, lora_rank) - ç”¨äº expand æ“ä½œ  
        weights_b = torch.randn(
            self. config.num_loras,
            self.config.hidden_dim,
            self.config.lora_rank,
            dtype=torch.float16,
            device=self.device
        )
        
        print(f"âœ“ Created LoRA weights:")
        print(f"  weights_a shape: {weights_a.shape}")
        print(f"  weights_b shape: {weights_b.shape}")
        
        return weights_a, weights_b
    
    def _create_input(self) -> torch.Tensor:
        """åˆ›å»ºè¾“å…¥å¼ é‡"""
        total_tokens = self. config.batch_size * self. config.seq_len
        x = torch.randn(
            total_tokens,
            self.config.hidden_dim,
            dtype=torch.float16,
            device=self.device
        )
        return x
    
    def _prepare_backend_batch_info(self, weight_indices: List[int]):
        """å‡†å¤‡åç«¯çš„æ‰¹æ¬¡ä¿¡æ¯"""
        # åˆ›å»ºåºåˆ—é•¿åº¦åˆ—è¡¨ï¼ˆæ¯ä¸ªåºåˆ—çš„é•¿åº¦ï¼‰
        seq_lens_cpu = [self.config.seq_len] * self.config.batch_size
        
        # åˆ›å»º LoRA ranks å’Œ scalings
        lora_ranks = [self.config.lora_rank] * self.config.num_loras
        scalings = [1.0] * self.config.num_loras
        
        # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„ forward_batch å¯¹è±¡
        forward_batch = MockForwardBatch(self.config.batch_size, seq_lens_cpu)
        
        # ä½¿ç”¨åç«¯çš„ prepare_lora_batch æ–¹æ³•
        self.backend.prepare_lora_batch(
            forward_batch=forward_batch,
            weight_indices=weight_indices,
            lora_ranks=lora_ranks,
            scalings=scalings,
            batch_info=None
        )
    
    def _run_forward(self, x: torch.Tensor) -> torch.Tensor:
        """è¿è¡Œå‰å‘ä¼ æ’­ï¼šLoRA A + LoRA B"""
        # LoRA A (shrink): x @ A^T -> (total_tokens, lora_rank)
        lora_a_output = self.backend. run_lora_a_sgemm(x, self.weights_a)
        
        # LoRA B (expand): lora_a_output @ B^T -> (total_tokens, hidden_dim)
        output_offset = torch.tensor(
            [0, self.config.hidden_dim],
            dtype=torch.int32,
            device=self.device
        )
        
        output = self.backend.run_lora_b_sgemm(
            x=lora_a_output,
            weights=self.weights_b,
            output_offset=output_offset,
            base_output=None
        )
        
        return output
    
    def benchmark_scenario(
        self,
        weight_indices: List[int],
        scenario_name: str
    ) -> Dict[str, float]:
        """å¯¹ç‰¹å®šåœºæ™¯è¿›è¡ŒåŸºå‡†æµ‹è¯•
        
        Args: 
            weight_indices: æ¯ä¸ªåºåˆ—ä½¿ç”¨çš„ LoRA ç´¢å¼•
            scenario_name: åœºæ™¯åç§°ï¼ˆç”¨äºæ‰“å°ï¼‰
        
        Returns:
            åŒ…å«æ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
        """
        config = self.config
        
        # å‡†å¤‡æ‰¹æ¬¡ä¿¡æ¯
        self._prepare_backend_batch_info(weight_indices)
        
        # åˆ›å»ºè¾“å…¥
        x = self._create_input()
        
        print(f"  Running {scenario_name} scenario...")
        print(f"    Weight indices: {weight_indices[: min(8, len(weight_indices))]}{'...' if len(weight_indices) > 8 else ''}")
        
        # Warmup
        for i in range(config.warmup_iterations):
            try:
                output = self._run_forward(x)
                torch.cuda.synchronize()
            except Exception as e: 
                print(f"    Warmup iteration {i+1}/{config.warmup_iterations} failed: {e}")
                raise
        
        print(f"    Warmup completed ({config.warmup_iterations} iterations)")
        
        # æµ‹è¯•
        torch.cuda.synchronize()
        start_time = time. perf_counter()
        
        for i in range(config. test_iterations):
            try:
                output = self._run_forward(x)
            except Exception as e: 
                print(f"    Test iteration {i+1}/{config.test_iterations} failed: {e}")
                raise
        
        torch.cuda.synchronize()
        end_time = time.perf_counter()
        
        # è®¡ç®—æŒ‡æ ‡
        total_time = (end_time - start_time) * 1000  # ms
        avg_time = total_time / config.test_iterations  # ms per iteration
        throughput = (config.batch_size * config.test_iterations) / (total_time / 1000)  # requests/sec
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        if output is not None:
            expected_shape = (config.batch_size * config.seq_len, config.hidden_dim)
            if output.shape != expected_shape:
                print(f"    âš ï¸  Warning: Output shape {output.shape} != expected {expected_shape}")
        
        return {
            'scenario': scenario_name,
            'avg_time_ms': avg_time,
            'throughput_rps': throughput,
            'total_time_ms':  total_time
        }
    
    def run_comparison(self) -> Dict[str, any]:
        """è¿è¡ŒåŒä¸€ LoRA vs ä¸åŒ LoRA çš„å¯¹æ¯”æµ‹è¯•"""
        
        print("\n" + "=" * 80)
        print("Running Benchmark:  Same LoRA vs Different LoRAs")
        print("=" * 80)
        
        # åœºæ™¯ 1: æ‰€æœ‰è¯·æ±‚ä½¿ç”¨åŒä¸€ä¸ª LoRA (lora_0)
        print("\n[Scenario 1] All requests use the SAME LoRA (lora_0)")
        weight_indices_same = [0] * self.config. batch_size
        result_same = self. benchmark_scenario(weight_indices_same, "Same LoRA")
        print(f"  Average time: {result_same['avg_time_ms']:.4f} ms")
        print(f"  Throughput:  {result_same['throughput_rps']:.2f} requests/sec")
        
        # åœºæ™¯ 2: æ¯ä¸ªè¯·æ±‚ä½¿ç”¨ä¸åŒçš„ LoRA
        print("\n[Scenario 2] Each request uses a DIFFERENT LoRA")
        weight_indices_diff = [i % self.config. num_loras for i in range(self.config.batch_size)]
        result_diff = self. benchmark_scenario(weight_indices_diff, "Different LoRAs")
        print(f"  Average time: {result_diff['avg_time_ms']:.4f} ms")
        print(f"  Throughput:  {result_diff['throughput_rps']:.2f} requests/sec")
        
        # è®¡ç®—å·®å¼‚
        if result_same['avg_time_ms'] > 0:
            speedup = result_same['avg_time_ms'] / result_diff['avg_time_ms']
            overhead_pct = (result_diff['avg_time_ms'] - result_same['avg_time_ms']) / result_same['avg_time_ms'] * 100
        else:
            speedup = 1.0
            overhead_pct = 0.0
        
        print("\n" + "=" * 80)
        print("Results Summary")
        print("=" * 80)
        print(f"Same LoRA time:            {result_same['avg_time_ms']:.4f} ms")
        print(f"Different LoRAs time:      {result_diff['avg_time_ms']:.4f} ms")
        if speedup > 1:
            print(f"Performance ratio:        {speedup:.3f}x (Same is faster)")
        else:
            print(f"Performance ratio:        {1/speedup:.3f}x (Different is faster)")
        print(f"Overhead:                  {overhead_pct:+.2f}%")
        print("=" * 80)
        
        return {
            'same_lora': result_same,
            'different_loras': result_diff,
            'speedup': speedup,
            'overhead_pct': overhead_pct
        }
    
    def run_batch_size_sweep(self, batch_sizes: List[int]) -> List[Dict]: 
        """æ‰«æä¸åŒçš„ batch size"""
        
        print("\n" + "=" * 80)
        print("Batch Size Sweep")
        print("=" * 80)
        
        results = []
        original_batch_size = self.config.batch_size
        
        for bs in batch_sizes: 
            print(f"\nTesting batch size: {bs}")
            self.config.batch_size = bs
            
            # åœºæ™¯ 1: åŒä¸€ LoRA
            weight_indices_same = [0] * bs
            result_same = self.benchmark_scenario(weight_indices_same, f"Same-{bs}")
            
            # åœºæ™¯ 2: ä¸åŒ LoRA
            weight_indices_diff = [i % self.config.num_loras for i in range(bs)]
            result_diff = self.benchmark_scenario(weight_indices_diff, f"Diff-{bs}")
            
            if result_same['avg_time_ms'] > 0:
                overhead_pct = (result_diff['avg_time_ms'] - result_same['avg_time_ms']) / result_same['avg_time_ms'] * 100
            else: 
                overhead_pct = 0.0
            
            results.append({
                'batch_size':  bs,
                'same_lora_ms': result_same['avg_time_ms'],
                'diff_lora_ms': result_diff['avg_time_ms'],
                'overhead_pct': overhead_pct
            })
            
            print(f"  Same LoRA:         {result_same['avg_time_ms']:.4f} ms")
            print(f"  Different LoRA:   {result_diff['avg_time_ms']:.4f} ms")
            print(f"  Overhead:         {overhead_pct:+.2f}%")
        
        # æ¢å¤åŸå§‹ batch size
        self.config.batch_size = original_batch_size
        
        return results
    
    def plot_results(self, sweep_results: List[Dict], save_path: str = "lora_benchmark. png"):
        """ç»˜åˆ¶ç»“æœå›¾è¡¨"""
        
        if not sweep_results: 
            print("âš ï¸  No results to plot")
            return
            
        batch_sizes = [r['batch_size'] for r in sweep_results]
        same_lora_times = [r['same_lora_ms'] for r in sweep_results]
        diff_lora_times = [r['diff_lora_ms'] for r in sweep_results]
        overheads = [r['overhead_pct'] for r in sweep_results]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
        
        # å­å›¾ 1: æ—¶é—´å¯¹æ¯”
        ax1.plot(batch_sizes, same_lora_times, 'o-', label='Same LoRA', linewidth=2, markersize=8, color='blue')
        ax1.plot(batch_sizes, diff_lora_times, 's-', label='Different LoRAs', linewidth=2, markersize=8, color='red')
        ax1.set_xlabel('Batch Size', fontsize=12)
        ax1.set_ylabel('Average Time (ms)', fontsize=12)
        ax1.set_title('LoRA SGMV Kernel Performance', fontsize=14, fontweight='bold')
        ax1.legend(fontsize=11)
        ax1.grid(True, alpha=0.3)
        ax1.set_xscale('log', base=2)
        
        # å­å›¾ 2: å¼€é”€ç™¾åˆ†æ¯”
        ax2.plot(batch_sizes, overheads, 'D-', color='red', linewidth=2, markersize=8)
        ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)
        ax2.set_xlabel('Batch Size', fontsize=12)
        ax2.set_ylabel('Overhead (%)', fontsize=12)
        ax2.set_title('Performance Overhead (Different vs Same)', fontsize=14, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        ax2.set_xscale('log', base=2)
        
        plt.tight_layout()
        
        try:
            plt. savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"\nâœ“ Plot saved to:  {save_path}")
        except Exception as e:
            print(f"\nâš ï¸  Failed to save plot: {e}")
        
        plt.close()


def main():
    """ä¸»å‡½æ•°"""
    
    # é…ç½®
    config = BenchmarkConfig(
        batch_size=8,
        seq_len=128,
        hidden_dim=4096,
        lora_rank=64,
        num_loras=4,
        warmup_iterations=5,   # å‡å°‘ warmup è¿­ä»£æ¬¡æ•°
        test_iterations=50,    # å‡å°‘æµ‹è¯•è¿­ä»£æ¬¡æ•°ä»¥åŠ å¿«è°ƒè¯•
        device="cuda:1",       # æ ¹æ®ä½ çš„å®é™…æƒ…å†µè°ƒæ•´
        max_chunk_size=128
    )
    
    try:
        # åˆ›å»ºåŸºå‡†æµ‹è¯•
        benchmark = LoRABenchmark(config)
        
        # è¿è¡Œä¸»è¦å¯¹æ¯”æµ‹è¯•
        comparison_results = benchmark. run_comparison()
        
        # è¿è¡Œ batch size æ‰«æ  
        batch_sizes = [4, 8, 16, 32]  # å‡å°‘æµ‹è¯•çš„ batch size æ•°é‡
        sweep_results = benchmark. run_batch_size_sweep(batch_sizes)
        
        # ç»˜åˆ¶ç»“æœ
        benchmark.plot_results(sweep_results)
        
        print("\n" + "=" * 80)
        print("ğŸ‰ Benchmark Complete!")
        print("=" * 80)
        
        # æ‰“å°å…³é”®å‘ç°
        overhead = comparison_results['overhead_pct']
        if overhead > 5:
            print(f"ğŸ’¡ Key Finding: Different LoRAs have {overhead:.1f}% overhead compared to same LoRA")
        elif overhead > 0:
            print(f"ğŸ’¡ Key Finding:  Different LoRAs have minimal {overhead:.1f}% overhead")
        else:
            print(f"ğŸ’¡ Key Finding: Different LoRAs are actually {abs(overhead):.1f}% faster (unexpected!)")
        
    except Exception as e: 
        print(f"\nâŒ Benchmark failed: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":  
    exit_code = main()
    sys.exit(exit_code)