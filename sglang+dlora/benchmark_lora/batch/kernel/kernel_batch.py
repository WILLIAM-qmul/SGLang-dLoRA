"""
æœªåˆå¹¶æ¨ç†æ€§èƒ½å¯¹æ¯”æµ‹è¯•ï¼šTriton vs Chunked SGMV åç«¯
æµ‹è¯•åœºæ™¯ï¼šWX + BAXï¼ˆå®Œæ•´çš„æœªåˆå¹¶æ¨ç†æµç¨‹ï¼‰
å¯¹æ¯”ç»´åº¦ï¼šåŒä¸€LoRA vs ä¸åŒLoRAï¼Œä»¥åŠä¸åŒåç«¯çš„æ€§èƒ½
"""

import torch
import time
from typing import List, Dict, Tuple
from dataclasses import dataclass
from enum import Enum

from sglang.srt.lora.backend.chunked_backend import ChunkedSgmvLoRABackend
from sglang.srt.lora.backend.triton_backend import TritonLoRABackend
from sglang.srt.lora.backend.base_backend import BaseLoRABackend


class BackendType(Enum):
    """åç«¯ç±»å‹æšä¸¾"""
    TRITON = "triton"
    CHUNKED_SGMV = "csgmv"


@dataclass
class UnmergedConfig: 
    """æœªåˆå¹¶æ¨ç†æµ‹è¯•é…ç½®"""
    batch_size: int = 16
    seq_len: int = 128
    hidden_dim: int = 1024
    lora_rank:  int = 32
    num_loras: int = 4
    test_iterations: int = 1000
    device: str = "cuda:1"
    max_chunk_size: int = 8


class MinimalServerArgs:
    def __init__(self, max_lora_chunk_size: int = 32):
        self.max_lora_chunk_size = max_lora_chunk_size
        self.model_path = "/tmp/dummy"


class SimpleForwardMode: 
    def is_extend(self) -> bool:
        return True


class SimpleForwardBatch:
    def __init__(self, batch_size: int, seq_len: int, device: torch.device):
        self.batch_size = batch_size
        self.extend_seq_lens_cpu = [seq_len] * batch_size
        self.forward_mode = SimpleForwardMode()
        self.extend_num_tokens = batch_size * seq_len
        # Triton backend éœ€è¦ extend_seq_lens tensor
        self.extend_seq_lens = torch.tensor(
            self.extend_seq_lens_cpu, 
            dtype=torch.int32, 
            device=device
        )


class UnmergedLoRAInferenceTester:
    """æœªåˆå¹¶LoRAæ¨ç†æµ‹è¯•å™¨ - æ”¯æŒå¤šåç«¯"""
    
    def __init__(self, config: UnmergedConfig, backend_type: BackendType):
        self.config = config
        self.backend_type = backend_type
        self.device = torch.device(config.device)
        torch.cuda.set_device(self.device)
        
        # åˆå§‹åŒ–åç«¯
        self. backend = self._create_backend()
        
        # åˆ›å»ºæƒé‡
        self.base_weight = self._create_base_weight()
        self.lora_weights_a, self.lora_weights_b = self._create_lora_weights()
        
        print(f"âœ“ {backend_type.value. upper()} Backend Tester initialized")
        print(f"  Base weight shape: {self.base_weight. shape}")
        print(f"  LoRA A shape: {self.lora_weights_a.shape}")
        print(f"  LoRA B shape: {self.lora_weights_b.shape}")
    
    def _create_backend(self) -> BaseLoRABackend:
        """æ ¹æ®ç±»å‹åˆ›å»ºåç«¯"""
        if self.backend_type == BackendType.TRITON:
            return TritonLoRABackend(
                max_loras_per_batch=self.config.num_loras,
                device=self.device
            )
        elif self.backend_type == BackendType.CHUNKED_SGMV:
            server_args = MinimalServerArgs(self.config.max_chunk_size)
            return ChunkedSgmvLoRABackend(
                max_loras_per_batch=self.config. num_loras,
                device=self.device,
                server_args=server_args
            )
        else:
            raise ValueError(f"Unknown backend type:  {self.backend_type}")
    
    def _create_base_weight(self) -> torch.Tensor:
        """åˆ›å»ºåŸºç¡€æ¨¡å‹æƒé‡ W"""
        return torch.randn(
            self.config.hidden_dim,
            self.config.hidden_dim,
            dtype=torch.float16,
            device=self.device
        )
    
    def _create_lora_weights(self) -> Tuple[torch.Tensor, torch.Tensor]:
        """åˆ›å»º LoRA æƒé‡ A å’Œ B"""
        weights_a = torch. randn(
            self.config.num_loras,
            self.config.lora_rank,
            self.config.hidden_dim,
            dtype=torch.float16,
            device=self.device
        )
        
        weights_b = torch.randn(
            self.config. num_loras,
            self.config.hidden_dim,
            self.config.lora_rank,
            dtype=torch.float16,
            device=self.device
        )
        
        return weights_a, weights_b
    
    def _create_input(self) -> torch.Tensor:
        """åˆ›å»ºè¾“å…¥æ•°æ®"""
        total_tokens = self.config. batch_size * self.config. seq_len
        return torch. randn(
            total_tokens,
            self.config.hidden_dim,
            dtype=torch.float16,
            device=self.device
        )
    
    def _prepare_backend(self, weight_indices: List[int]):
        """å‡†å¤‡åç«¯æ‰¹æ¬¡ä¿¡æ¯"""
        forward_batch = SimpleForwardBatch(
            self.config.batch_size, 
            self.config.seq_len,
            self.device
        )
        lora_ranks = [self.config.lora_rank] * self.config.num_loras
        scalings = [1.0] * self.config.num_loras
        
        self.backend.prepare_lora_batch(
            forward_batch=forward_batch,
            weight_indices=weight_indices,
            lora_ranks=lora_ranks,
            scalings=scalings,
            batch_info=None
        )
    
    def unmerged_inference(self, x: torch.Tensor) -> torch.Tensor:
        """
        å®Œæ•´çš„æœªåˆå¹¶æ¨ç†ï¼šWX + BAX
        è¿™æ˜¯ SGLang ä¸­å®é™…ä½¿ç”¨çš„æ–¹å¼
        """
        # 1. åŸºç¡€æ¨¡å‹è®¡ç®—ï¼šWX
        base_output = torch.mm(x, self.base_weight. T)
        
        # 2. LoRA è®¡ç®—ï¼šBAX (ä½¿ç”¨ backend kernel)
        # LoRA A (shrink): x @ A^T -> (total_tokens, lora_rank)
        lora_a_output = self.backend.run_lora_a_sgemm(x, self.lora_weights_a)
        
        # LoRA B (expand): lora_a_output @ B^T -> (total_tokens, hidden_dim)
        if self.backend_type == BackendType.TRITON:
            # Triton backend ä¸éœ€è¦ output_offset
            lora_output = self.backend.run_lora_b_sgemm(
                x=lora_a_output,
                weights=self.lora_weights_b,
                base_output=base_output
            )
        else:
            # Chunked SGMV backend éœ€è¦ output_offset
            output_offset = torch.tensor(
                [0, self.config. hidden_dim], 
                dtype=torch.int32, 
                device=x.device
            )
            lora_output = self.backend. run_lora_b_sgemm(
                x=lora_a_output,
                weights=self.lora_weights_b,
                output_offset=output_offset,
                base_output=base_output
            )
        
        return lora_output
    
    def benchmark_unmerged_inference(
        self, 
        weight_indices: List[int], 
        scenario_name: str
    ) -> Dict[str, float]:
        """æµ‹è¯•æœªåˆå¹¶æ¨ç†æ€§èƒ½"""
        print(f"\nğŸ”„ Testing {scenario_name}")
        print(f"   Backend: {self.backend_type. value. upper()}")
        print(f"   Weight indices: {weight_indices[:8]}{'...' if len(weight_indices) > 8 else ''}")
        
        # å‡†å¤‡åç«¯
        self._prepare_backend(weight_indices)
        
        # åˆ›å»ºè¾“å…¥
        x = self._create_input()
        
        # Warmup
        try:
            for _ in range(10):  # å¤šæ¬¡warmupç¡®ä¿ç¨³å®š
                _ = self.unmerged_inference(x)
            torch.cuda.synchronize()
            print("   âœ“ Warmup completed")
        except Exception as e:
            print(f"   âŒ Warmup failed: {e}")
            raise
        
        # æ€§èƒ½æµ‹è¯•
        torch.cuda.synchronize()
        start_time = time.perf_counter()
        
        for _ in range(self.config. test_iterations):
            _ = self.unmerged_inference(x)
        
        torch. cuda.synchronize()
        end_time = time.perf_counter()
        
        # è®¡ç®—æŒ‡æ ‡
        total_time_ms = (end_time - start_time) * 1000
        avg_time_ms = total_time_ms / self.config.test_iterations
        avg_time_us = avg_time_ms * 1000
        throughput = (self.config.batch_size * self.config.test_iterations) / (total_time_ms / 1000)
        
        print(f"   â±ï¸  Average time: {avg_time_us:.1f} us")
        print(f"   ğŸš€ Throughput: {throughput:.1f} requests/sec")
        
        return {
            'scenario': scenario_name,
            'backend': self.backend_type.value,
            'avg_time_us': avg_time_us,
            'throughput_rps': throughput,
            'weight_indices': weight_indices. copy()
        }


class MultiBackendComparison:
    """å¤šåç«¯å¯¹æ¯”æµ‹è¯•ç®¡ç†å™¨"""
    
    def __init__(self, config: UnmergedConfig):
        self.config = config
        self.testers = {
            BackendType.TRITON: UnmergedLoRAInferenceTester(config, BackendType.TRITON),
            BackendType.CHUNKED_SGMV: UnmergedLoRAInferenceTester(config, BackendType.CHUNKED_SGMV)
        }
        self.results = {}
    
    def run_backend_comparison(self, backend_type: BackendType) -> Dict: 
        """è¿è¡Œå•ä¸ªåç«¯çš„å¯¹æ¯”æµ‹è¯•"""
        print(f"\n{'='*70}")
        print(f"ğŸ”¬ æµ‹è¯•åç«¯: {backend_type.value.upper()}")
        print(f"{'='*70}")
        
        tester = self.testers[backend_type]
        
        # åœºæ™¯ 1: æ‰€æœ‰è¯·æ±‚ä½¿ç”¨åŒä¸€ä¸ª LoRA
        print(f"\nğŸ“Š åœºæ™¯ 1: æ‰€æœ‰ {self.config.batch_size} ä¸ªè¯·æ±‚éƒ½ä½¿ç”¨ LoRA_0")
        weight_indices_same = [0] * self.config.batch_size
        result_same = tester.benchmark_unmerged_inference(
            weight_indices_same, 
            f"Same LoRA (all lora_0)"
        )
        
        # åœºæ™¯ 2: æ¯ä¸ªè¯·æ±‚ä½¿ç”¨ä¸åŒçš„ LoRA
        print(f"\nğŸ“Š åœºæ™¯ 2: æ¯ä¸ªè¯·æ±‚ä½¿ç”¨ä¸åŒçš„ LoRA")
        weight_indices_diff = [
            i % self.config.num_loras 
            for i in range(self.config.batch_size)
        ]
        result_diff = tester.benchmark_unmerged_inference(
            weight_indices_diff, 
            f"Different LoRAs (mixed)"
        )
        
        # è®¡ç®—æ€§èƒ½å·®å¼‚
        overhead_pct = 0
        throughput_loss = 0
        if result_same['avg_time_us'] > 0:
            overhead_pct = (
                (result_diff['avg_time_us'] - result_same['avg_time_us']) 
                / result_same['avg_time_us'] * 100
            )
            throughput_loss = (
                (result_same['throughput_rps'] - result_diff['throughput_rps']) 
                / result_same['throughput_rps'] * 100
            )
        
        return {
            'backend': backend_type.value,
            'same_lora':  result_same,
            'different_lora': result_diff,
            'overhead_pct':  overhead_pct,
            'throughput_loss_pct': throughput_loss
        }
    
    def run_all_backends_comparison(self):
        """è¿è¡Œæ‰€æœ‰åç«¯çš„å¯¹æ¯”æµ‹è¯•"""
        print("\n" + "ğŸš€ å¤šåç«¯æœªåˆå¹¶æ¨ç†æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
        print("=" * 70)
        print("æµ‹è¯•å†…å®¹ï¼šå®Œæ•´çš„æœªåˆå¹¶æ¨ç† WX + BAX")
        print("æµ‹è¯•åç«¯ï¼šTriton vs Chunked SGMV")
        print("å¯¹æ¯”åœºæ™¯ï¼šåŒä¸€LoRA vs ä¸åŒLoRA")
        print("=" * 70)
        
        # æµ‹è¯•æ¯ä¸ªåç«¯
        for backend_type in [BackendType.TRITON, BackendType.CHUNKED_SGMV]:
            self.results[backend_type] = self.run_backend_comparison(backend_type)
        
        # ç»¼åˆåˆ†æ
        self._print_comprehensive_analysis()
    
    def _print_comprehensive_analysis(self):
        """æ‰“å°ç»¼åˆåˆ†æç»“æœ"""
        print("\n" + "=" * 70)
        print("ğŸ“Š ç»¼åˆæ€§èƒ½åˆ†æ")
        print("=" * 70)
        
        # 1. å„åç«¯çš„åŸºæœ¬æ€§èƒ½æŒ‡æ ‡
        print("\n1ï¸âƒ£ å„åç«¯æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”")
        print("-" * 70)
        
        for backend_type in [BackendType. TRITON, BackendType. CHUNKED_SGMV]: 
            result = self.results[backend_type]
            print(f"\nã€{backend_type.value.upper()} Backendã€‘")
            
            print(f"  åŒä¸€LoRAåœºæ™¯:")
            print(f"    å¹³å‡æ—¶é—´:   {result['same_lora']['avg_time_us']:.1f} us")
            print(f"    ååé‡:    {result['same_lora']['throughput_rps']:.1f} req/s")
            
            print(f"  ä¸åŒLoRAåœºæ™¯:")
            print(f"    å¹³å‡æ—¶é—´:  {result['different_lora']['avg_time_us']:.1f} us")
            print(f"    ååé‡:    {result['different_lora']['throughput_rps']:.1f} req/s")
            
            print(f"  æ€§èƒ½å·®å¼‚:")
            print(f"    æ—¶é—´å¼€é”€:   {result['overhead_pct']:+.2f}%")
            print(f"    ååæŸå¤±:  {result['throughput_loss_pct']:+.2f}%")
        
        # 2. åç«¯é—´çš„æ€§èƒ½å¯¹æ¯”
        print("\n2ï¸âƒ£ åç«¯é—´æ€§èƒ½å¯¹æ¯”")
        print("-" * 70)
        
        triton_result = self.results[BackendType.TRITON]
        chunked_result = self.results[BackendType.CHUNKED_SGMV]
        
        # åŒä¸€LoRAåœºæ™¯å¯¹æ¯”
        same_lora_speedup = (
            chunked_result['same_lora']['avg_time_us'] 
            / triton_result['same_lora']['avg_time_us']
        )
        print(f"\nåŒä¸€LoRAåœºæ™¯:")
        print(f"  Triton:         {triton_result['same_lora']['avg_time_us']:.1f} us")
        print(f"  Chunked SGMV:   {chunked_result['same_lora']['avg_time_us']:.1f} us")
        print(f"  åŠ é€Ÿæ¯”:         {same_lora_speedup:.2f}x " 
              f"({'Tritonæ›´å¿«' if same_lora_speedup > 1 else 'Chunkedæ›´å¿«'})")
        
        # ä¸åŒLoRAåœºæ™¯å¯¹æ¯”
        diff_lora_speedup = (
            chunked_result['different_lora']['avg_time_us'] 
            / triton_result['different_lora']['avg_time_us']
        )
        print(f"\nä¸åŒLoRAåœºæ™¯:")
        print(f"  Triton:         {triton_result['different_lora']['avg_time_us']:.1f} us")
        print(f"  Chunked SGMV:  {chunked_result['different_lora']['avg_time_us']:.1f} us")
        print(f"  åŠ é€Ÿæ¯”:        {diff_lora_speedup:.2f}x "
              f"({'Tritonæ›´å¿«' if diff_lora_speedup > 1 else 'Chunkedæ›´å¿«'})")
        
        # 3. å…³é”®å‘ç°
        print("\n3ï¸âƒ£ å…³é”®å‘ç°")
        print("-" * 70)
        
        triton_overhead = triton_result['overhead_pct']
        chunked_overhead = chunked_result['overhead_pct']
        
        print(f"\nğŸ“Œ ä¸åŒLoRAå¸¦æ¥çš„æ€§èƒ½å¼€é”€:")
        print(f"  Triton:        {triton_overhead:+.2f}%")
        print(f"  Chunked SGMV:  {chunked_overhead:+.2f}%")
        
        if abs(triton_overhead - chunked_overhead) > 5:
            better_backend = "Triton" if triton_overhead < chunked_overhead else "Chunked SGMV"
            worse_backend = "Chunked SGMV" if better_backend == "Triton" else "Triton"
            print(f"\n  ğŸ’¡ {better_backend} åœ¨å¤„ç†ä¸åŒLoRAæ—¶çš„æ€§èƒ½é€€åŒ–æ›´å°")
            print(f"     {better_backend} å¯¹ LoRA å¤šæ ·æ€§çš„å¤„ç†æ›´ä¼˜")
        else:
            print(f"\n  âœ… ä¸¤ç§åç«¯åœ¨å¤„ç†ä¸åŒLoRAæ—¶çš„æ€§èƒ½é€€åŒ–ç›¸è¿‘")
        
        # 4. ä½¿ç”¨å»ºè®®
        print("\n4ï¸âƒ£ ä½¿ç”¨å»ºè®®")
        print("-" * 70)
        
        if same_lora_speedup > 1.2:
            print(f"â€¢ å¯¹äºåŒä¸€LoRAæ‰¹æ¬¡ï¼šä¼˜å…ˆé€‰æ‹© Triton (å¿« {(same_lora_speedup-1)*100:.1f}%)")
        elif same_lora_speedup < 0.8:
            print(f"â€¢ å¯¹äºåŒä¸€LoRAæ‰¹æ¬¡ï¼šä¼˜å…ˆé€‰æ‹© Chunked SGMV (å¿« {(1/same_lora_speedup-1)*100:.1f}%)")
        else:
            print(f"â€¢ å¯¹äºåŒä¸€LoRAæ‰¹æ¬¡ï¼šä¸¤ç§åç«¯æ€§èƒ½ç›¸è¿‘ï¼Œå¯ä»»é€‰")
        
        if diff_lora_speedup > 1.2:
            print(f"â€¢ å¯¹äºæ··åˆLoRAæ‰¹æ¬¡ï¼šä¼˜å…ˆé€‰æ‹© Triton (å¿« {(diff_lora_speedup-1)*100:.1f}%)")
        elif diff_lora_speedup < 0.8:
            print(f"â€¢ å¯¹äºæ··åˆLoRAæ‰¹æ¬¡ï¼šä¼˜å…ˆé€‰æ‹© Chunked SGMV (å¿« {(1/diff_lora_speedup-1)*100:.1f}%)")
        else:
            print(f"â€¢ å¯¹äºæ··åˆLoRAæ‰¹æ¬¡ï¼šä¸¤ç§åç«¯æ€§èƒ½ç›¸è¿‘ï¼Œå¯ä»»é€‰")
        
        if triton_overhead > 10 or chunked_overhead > 10:
            print(f"â€¢ âš ï¸  ä¸åŒLoRAå¸¦æ¥çš„æ€§èƒ½å¼€é”€è¾ƒå¤§ï¼Œå»ºè®®ä½¿ç”¨dLoRAç­‰ä¼˜åŒ–ç­–ç•¥")
        
        # 5. æµ‹è¯•é…ç½®
        print("\n5ï¸âƒ£ æµ‹è¯•é…ç½®")
        print("-" * 70)
        print(f"  æ‰¹æ¬¡å¤§å°:       {self.config.batch_size}")
        print(f"  åºåˆ—é•¿åº¦:      {self.config. seq_len}")
        print(f"  æ€»tokenæ•°:     {self.config.batch_size * self.config.seq_len}")
        print(f"  LoRAæ•°é‡:      {self.config.num_loras}")
        print(f"  LoRAç»´åº¦:      {self.config. lora_rank}")
        print(f"  éšè—å±‚ç»´åº¦:    {self.config.hidden_dim}")
        print(f"  æµ‹è¯•è¿­ä»£æ¬¡æ•°:   {self.config.test_iterations}")
        
        print("=" * 70)
    
    def run_batch_size_analysis(self):
        """åˆ†æä¸åŒbatch sizeä¸‹å„åç«¯çš„æ€§èƒ½"""
        print("\nğŸ”¬ Batch Size å½±å“åˆ†æ (å¤šåç«¯)")
        print("=" * 70)
        
        original_batch_size = self.config.batch_size
        batch_sizes = [4, 8, 16, 32]
        
        all_results = {
            BackendType.TRITON: [],
            BackendType.CHUNKED_SGMV: []
        }
        
        for bs in batch_sizes:
            print(f"\n{'='*60}")
            print(f"æµ‹è¯• Batch Size: {bs}")
            print(f"{'='*60}")
            
            self.config.batch_size = bs
            
            # ä¸ºæ¯ä¸ªåç«¯æµ‹è¯•
            for backend_type in [BackendType.TRITON, BackendType.CHUNKED_SGMV]:
                print(f"\n{backend_type.value.upper()} Backend:")
                tester = self.testers[backend_type]
                
                # åŒä¸€LoRA
                weight_indices_same = [0] * bs
                result_same = tester.benchmark_unmerged_inference(
                    weight_indices_same, 
                    f"Same-BS{bs}"
                )
                
                # ä¸åŒLoRA
                weight_indices_diff = [i % self.config.num_loras for i in range(bs)]
                result_diff = tester.benchmark_unmerged_inference(
                    weight_indices_diff, 
                    f"Diff-BS{bs}"
                )
                
                overhead = (
                    (result_diff['avg_time_us'] - result_same['avg_time_us']) 
                    / result_same['avg_time_us'] * 100
                )
                
                all_results[backend_type].append({
                    'batch_size': bs,
                    'same_time_us': result_same['avg_time_us'],
                    'diff_time_us': result_diff['avg_time_us'],
                    'overhead_pct':  overhead
                })
        
        # æ¢å¤åŸå§‹é…ç½®
        self.config.batch_size = original_batch_size
        
        # æ‰“å°åˆ†ææ€»ç»“
        print(f"\n{'='*70}")
        print("ğŸ“Š Batch Size åˆ†ææ€»ç»“")
        print(f"{'='*70}")
        
        print(f"\n{'Batch Size':<12} {'Backend':<12} {'Same LoRA':<15} {'Diff LoRA':<15} {'Overhead': <10}")
        print("-" * 70)
        
        for bs in batch_sizes:
            for backend_type in [BackendType.TRITON, BackendType.CHUNKED_SGMV]:
                result = next(
                    r for r in all_results[backend_type] 
                    if r['batch_size'] == bs
                )
                print(f"{bs:<12} {backend_type.value. upper():<12} "
                      f"{result['same_time_us']:<15.1f} "
                      f"{result['diff_time_us']:<15.1f} "
                      f"{result['overhead_pct']: >+7.1f}%")
        
        print("=" * 70)
        
        return all_results


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    
    # é…ç½®
    config = UnmergedConfig(
        batch_size=8,
        seq_len=512,
        hidden_dim=4096,
        lora_rank=64,
        num_loras=4,
        test_iterations=100000,
        device="cuda:1",
        max_chunk_size=16
    )
    
    print("ğŸ”¬ SGLang å¤šåç«¯æœªåˆå¹¶æ¨ç†æ€§èƒ½æµ‹è¯•")
    print("=" * 70)
    print("ç›®æ ‡ï¼šå¯¹æ¯” Triton å’Œ Chunked SGMV ä¸¤ç§åç«¯çš„æ€§èƒ½")
    print("åœºæ™¯ï¼šåŒä¸€LoRA vs ä¸åŒLoRA")
    print("æ–¹æ³•ï¼šå®Œæ•´çš„æœªåˆå¹¶æ¨ç† (WX + BAX)")
    print("=" * 70)
    
    try:
        # åˆ›å»ºå¤šåç«¯å¯¹æ¯”ç®¡ç†å™¨
        comparison = MultiBackendComparison(config)
        
        # è¿è¡Œä¸»è¦å¯¹æ¯”æµ‹è¯•
        comparison.run_all_backends_comparison()
        
        # # è¿è¡Œbatch sizeåˆ†æ
        # print("\n" + "="*70)
        # print("è¿›è¡Œ Batch Size å½±å“åˆ†æ...")
        # print("="*70)
        # batch_analysis = comparison.run_batch_size_analysis()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__": 
    exit_code = main()
    exit(exit_code)